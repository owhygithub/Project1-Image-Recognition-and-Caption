{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FmLTZXJRbwP"
      },
      "source": [
        "## Image Recognition and Object Detection\n",
        "\n",
        "ImageNet + ObjectNet + Places Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-2czXWSjsYn",
        "outputId": "f517ed50-d14f-4bbf-9c26-e4b9f2522ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tinyimagenet in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.9)\n",
            "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tinyimagenet) (0.15.2)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->tinyimagenet) (1.24.3)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->tinyimagenet) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->tinyimagenet) (2.0.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->tinyimagenet) (9.5.0)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.0.1->torchvision->tinyimagenet) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.0.1->torchvision->tinyimagenet) (4.6.3)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.0.1->torchvision->tinyimagenet) (1.12)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.0.1->torchvision->tinyimagenet) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.0.1->torchvision->tinyimagenet) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision->tinyimagenet) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision->tinyimagenet) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision->tinyimagenet) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision->tinyimagenet) (2023.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch==2.0.1->torchvision->tinyimagenet) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch==2.0.1->torchvision->tinyimagenet) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!pip install tinyimagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qDG_wYnA_MpU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "from tinyimagenet import TinyImageNet\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnOiSe3POIae"
      },
      "source": [
        "#### What next?\n",
        "\n",
        "PLAN:\n",
        "1. Setup and load data\n",
        "2. Preprocessing data\n",
        "3. Building Neural Network Model\n",
        "4. Evaluation\n",
        "5. ChatGPT / or other GPT API\n",
        "6. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kxUJyl0jqOq",
        "outputId": "fe1cca85-9d17-486d-8010-f5165e30022d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TinyImageNet, split validation, has  10000 samples.\n",
            "TinyImageNet, split train, has  100000 samples.\n",
            "TinyImageNet, split test, has  10000 samples.\n",
            "Showing info of 5 samples from the training split...\n",
            "Sample of class   0, image, words ['goldfish', ' Carassius auratus']\n",
            "Sample of class  40, image, words ['walking stick', ' walkingstick', ' stick insect']\n",
            "Sample of class  80, image, words ['bucket', ' pail']\n",
            "Sample of class 120, image, words ['miniskirt', ' mini']\n",
            "Sample of class 160, image, words ['teapot']\n",
            "Showing info of 5 samples from the validation split...\n",
            "Sample of class   0, image, words ['goldfish', ' Carassius auratus']\n",
            "Sample of class  40, image, words ['walking stick', ' walkingstick', ' stick insect']\n",
            "Sample of class  80, image, words ['bucket', ' pail']\n",
            "Sample of class 120, image, words ['miniskirt', ' mini']\n",
            "Sample of class 160, image, words ['teapot']\n"
          ]
        }
      ],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    # transforms.Resize(256),          # Resize images to a consistent size\n",
        "    # transforms.CenterCrop(224),      # Crop a central 224x224 region\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    # transforms.Normalize(\n",
        "    #     mean=[0.485, 0.456, 0.406],  # Normalize with ImageNet statistics\n",
        "    #     std=[0.229, 0.224, 0.225]\n",
        "    # ),\n",
        "])\n",
        "\n",
        "dataset_val = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"),split=\"val\", transform=transform)\n",
        "dataset_train = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"),split=\"train\", transform=transform)\n",
        "dataset_test = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"),split=\"test\", transform=transform)\n",
        "\n",
        "n_val = len(dataset_val)\n",
        "n_train = len(dataset_train)\n",
        "n_test = len(dataset_test)\n",
        "print(f\"TinyImageNet, split validation, has  {n_val} samples.\")\n",
        "print(f\"TinyImageNet, split train, has  {n_train} samples.\")\n",
        "print(f\"TinyImageNet, split test, has  {n_test} samples.\")\n",
        "\n",
        "n_samples = 5\n",
        "print(f\"Showing info of {n_samples} samples from the training split...\")\n",
        "for i in range(0, n_train, n_train // n_samples):\n",
        "    image, klass = dataset_train[i]\n",
        "    print(f\"Sample of class {klass:3d}, image, words {dataset_train.idx_to_words[klass]}\")\n",
        "\n",
        "print(f\"Showing info of {n_samples} samples from the validation split...\")\n",
        "for i in range(0, n_val, n_val // n_samples):\n",
        "    image, klass = dataset_val[i]\n",
        "    print(f\"Sample of class {klass:3d}, image, words {dataset_val.idx_to_words[klass]}\")\n",
        "\n",
        "##### DISTINCT OBJECTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YXa1T_r-jqMf"
      },
      "outputs": [],
      "source": [
        "# dataset = training images\n",
        "\n",
        "# dataset_train = dataset_train.data\n",
        "# data_from_testing = test_data.data\n",
        "\n",
        "# convert to Tensors\n",
        "# color_tensor = torch.tensor(dataset_train, dtype=torch.float32)\n",
        "\n",
        "# Creating grayscale and color datasets\n",
        "# gray_dataset = TensorDataset(gray_tensor, color_tensor)\n",
        "\n",
        "# creating dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hVJolMzQjpz2"
      },
      "outputs": [],
      "source": [
        "# VAE\n",
        "# CNN - Network\n",
        "class imageNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(imageNet, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(4, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(4, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # extracting features\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # Forward prop\n",
        "        x = self.conv_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqvdUbvZjpsY",
        "outputId": "95158ebb-ffcc-43b0-8a9c-51993ec9917c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "imageNet(\n",
              "  (conv_layers): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): AvgPool2d(kernel_size=4, stride=2, padding=0)\n",
              "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): AvgPool2d(kernel_size=4, stride=2, padding=0)\n",
              "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = imageNet()\n",
        "model.to(device) # move to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "upPAnC4VjpoM",
        "outputId": "00db99fd-bf89-45bb-f7fe-2bdd70373b83"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "can only concatenate str (not \"Tensor\") to str",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/owhy/Desktop/github/project1-image-recognition-and-caption/ImageNet_model.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owhy/Desktop/github/project1-image-recognition-and-caption/ImageNet_model.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owhy/Desktop/github/project1-image-recognition-and-caption/ImageNet_model.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/owhy/Desktop/github/project1-image-recognition-and-caption/ImageNet_model.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39minputs\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owhy/Desktop/github/project1-image-recognition-and-caption/ImageNet_model.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owhy/Desktop/github/project1-image-recognition-and-caption/ImageNet_model.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"Tensor\") to str"
          ]
        }
      ],
      "source": [
        "# LOSS FUNCTION and OPTIMIZER - difference between OG and NEW\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # learning rate\n",
        "\n",
        "# TRAINING loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        print(\"inputs\")\n",
        "        print(inputs)\n",
        "        print(\"labels\")\n",
        "        print(labels)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.view(64, 1, 1)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(gray_dataloader)}\")\n",
        "\n",
        "print(\"Training finished!\")\n",
        "\n",
        "# Saving the model\n",
        "torch.save(model.state_dict(), \"imagenet.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XdmtW8UQjpiT"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for inputs, labels in val_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OYERvAGHjpXp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8IHxALwjo1G"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aUKxoU9P05E"
      },
      "source": [
        "### loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "nhoa4MXe_TFU",
        "outputId": "8c1de10c-3db1-463f-fd11-a24c24f95244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 49283072/170498071 [00:00<00:02, 51174436.77it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-442adab578b0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_default_https_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_default_https_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_unverified_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" to \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_save_response_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_save_response_content\u001b[0;34m(content, destination, length)\u001b[0m\n\u001b[1;32m     35\u001b[0m ) -> None:\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_save_response_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import ssl\n",
        "# Disable SSL verification\n",
        "ssl._create_default_https_context = ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "train_data = CIFAR10(root='./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = CIFAR10(root='./datasets', train=False, download=False, transform=transforms.ToTensor())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP4MQPfpNrv9"
      },
      "outputs": [],
      "source": [
        "def rgb_to_gray(rgb_image):\n",
        "    r, g, b = rgb_image[:, :, 0], rgb_image[:, :, 1], rgb_image[:, :, 2] # separate into individual colors\n",
        "    gray_image = (r+g+b)/3 # take the mean - luminosity method\n",
        "    return gray_image.astype(np.uint8) # converting to 1 channel\n",
        "\n",
        "# Actual Grayscale\n",
        "# def rgb_to_gray(rgb_image):\n",
        "#     r, g, b = rgb_image[:, :, 0], rgb_image[:, :, 1], rgb_image[:, :, 2] # separate into individual colors\n",
        "#     #gray_image = 0.2989 * r + 0.5870 * g + 0.1140 * b # take the mean - luminosity method\n",
        "#     #gray_image = 0.2125 * r + 0.7154 * g + 0.0721 * b\n",
        "#     return gray_image.astype(np.uint8) # converting to 1 channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09vb1OHTZkFW"
      },
      "outputs": [],
      "source": [
        "# get grayscale and color data\n",
        "data_from_training = train_data.data\n",
        "data_from_testing = test_data.data\n",
        "\n",
        "gray_data = np.array([rgb_to_gray(rgb_image) for rgb_image in data_from_training])\n",
        "gray_data_testing = np.array([rgb_to_gray(rgb_image) for rgb_image in data_from_testing])\n",
        "\n",
        "# convert to Tensors\n",
        "gray_tensor = torch.tensor(gray_data, dtype=torch.float32).unsqueeze(1)\n",
        "color_tensor = torch.tensor(data_from_training, dtype=torch.float32) / 255.0\n",
        "\n",
        "# Creating grayscale and color datasets\n",
        "gray_dataset = TensorDataset(gray_tensor, color_tensor)\n",
        "\n",
        "# creating dataloaders\n",
        "gray_dataloader = DataLoader(gray_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
        "color_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QwzdNbrP05G"
      },
      "outputs": [],
      "source": [
        "index_to_visualize = 2\n",
        "\n",
        "for i in range(index_to_visualize):\n",
        "  original_image = Image.fromarray(data_from_training[i])\n",
        "  gray_image = Image.fromarray(gray_data[i], 'L')\n",
        "\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(original_image)\n",
        "  plt.title(\"Original (RGB)\")\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(gray_image, cmap =\"gray\")\n",
        "  plt.title(\"Grayscale\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eigRw5O-SaC8"
      },
      "outputs": [],
      "source": [
        "# CNN for colorizing\n",
        "class Colorizer3000(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Colorizer3000, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1), # 1 channel goes in\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # extracting features\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # extracting features\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), # extracting features\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.deconv_layers = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=1, padding=1), # 3 channels come out\n",
        "            nn.Sigmoid()  # Sigmoid --> values are in [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # Forward prop\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.deconv_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rHi4m33SaFm"
      },
      "outputs": [],
      "source": [
        "model = Colorizer3000()\n",
        "model.to(device) # move to device\n",
        "\n",
        "# LOSS FUNCTION and OPTIMIZER - difference between OG and NEW\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # learning rate\n",
        "\n",
        "# TRAINING loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0 # accumulates losses for display\n",
        "    for gray_images, color_images in gray_dataloader: # run through dataloader with both gray and color images\n",
        "        gray_images = gray_images.to(device) # put to device\n",
        "        color_images = color_images.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        predicted_color = model(gray_images) # colorisation process\n",
        "\n",
        "        # resizing output to match the size of color_images\n",
        "        predicted_color_resized = F.interpolate(predicted_color, size=(32, 32), mode='bilinear', align_corners=False)\n",
        "        color_images = color_images.permute(0, 3, 1, 2)  # (batch_size, 32, 32, 3) to (batch_size, 3, 32, 32)\n",
        "\n",
        "        # Calculate the loss - difference between predicted-color / original image\n",
        "        loss = criterion(predicted_color_resized, color_images)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step() # update params\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(gray_dataloader)}\")\n",
        "\n",
        "print(\"Training finished!\")\n",
        "\n",
        "# Saving the model\n",
        "torch.save(model.state_dict(), \"colorizer_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYJIBUrgVeBw"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = Colorizer3000()\n",
        "model.load_state_dict(torch.load(\"colorizer_model.pth\"))\n",
        "model.to(device)\n",
        "model.eval() # eval mode\n",
        "\n",
        "# Colorize 10 grayscale images and compare with original colored images\n",
        "num_samples_to_visualize = 10\n",
        "\n",
        "plt.figure(figsize=(6, 3 * num_samples_to_visualize))\n",
        "\n",
        "for i in range(num_samples_to_visualize):\n",
        "    # Convert individ. grayscale image to tensor and move to device\n",
        "    gray_image = gray_data_testing[i]\n",
        "    print(\"number of gray instances - \" + str(len(gray_data_testing)))\n",
        "    print(\"number of original instances - \" + str(len(data_from_testing)))\n",
        "    gray_image_tensor = torch.tensor(gray_image, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    # Colorize grayscale image\n",
        "    with torch.no_grad():\n",
        "        colorized_image_tensor = model(gray_image_tensor)\n",
        "\n",
        "    #### Convert the tensor back to numpy array and normalize to [0, 1]\n",
        "    colorized_image = colorized_image_tensor.squeeze(0).cpu().numpy()\n",
        "    colorized_image = (colorized_image - colorized_image.min()) / (colorized_image.max() - colorized_image.min())\n",
        "\n",
        "    original_image = data_from_testing[i] / 255.0  # Scale to [0, 1] -> B or W -> normalisation\n",
        "\n",
        "    # Plot the original\n",
        "    plt.subplot(num_samples_to_visualize, 2, 2 * i + 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title(\"Original (RGB)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot the colorized image\n",
        "    plt.subplot(num_samples_to_visualize, 2, 2 * i + 2)\n",
        "    plt.imshow(colorized_image.transpose(1, 2, 0))  # Transpose the tensor to (32, 32, 3)\n",
        "    plt.title(\"New Colorized\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgkjo32SaNC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olAIx3dUdCZg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv-aVVDqdCbP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIlfdzPfdCdQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ_FkKobSaRF"
      },
      "outputs": [],
      "source": [
        "# load modeling\n",
        "model = Colorizer3000()\n",
        "model.load_state_dict(torch.load(\"colorizer_model.pth\"))\n",
        "model.to(device)\n",
        "# model.eval()\n",
        "\n",
        "# Colorize a few grayscale images and compare them with the original colored images\n",
        "num_samples_to_visualize = 5\n",
        "\n",
        "plt.figure(figsize=(6, 3 * num_samples_to_visualize))\n",
        "\n",
        "for i in range(num_samples_to_visualize):\n",
        "    # Convert grayscale image to tensor and move to device\n",
        "    gray_image = gray_data[i]\n",
        "    gray_image_tensor = torch.tensor(gray_image, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    # Colorize the grayscale image\n",
        "    with torch.no_grad():\n",
        "        colorized_image_tensor = model(gray_image_tensor)\n",
        "\n",
        "    # Convert the tensor back to numpy array and normalize to [0, 1]\n",
        "    colorized_image = colorized_image_tensor.squeeze(0).cpu().numpy()\n",
        "    colorized_image = (colorized_image - colorized_image.min()) / (colorized_image.max() - colorized_image.min())\n",
        "\n",
        "    # Get the original colored image\n",
        "    original_image = data_from_training[i] / 255.0  # Scale to [0, 1]\n",
        "\n",
        "    # Plot the original colored image\n",
        "    #plt.figure(figsize=(8, 20))\n",
        "    plt.subplot(num_samples_to_visualize, 2, 2 * i + 1)\n",
        "    plt.imshow(original_image)\n",
        "    plt.title(\"Original (RGB)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot the colorized image\n",
        "    plt.subplot(num_samples_to_visualize, 2, 2 * i + 2)\n",
        "    plt.imshow(colorized_image.transpose(1, 2, 0))\n",
        "    plt.title(\"Colorized Attempt\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9MSFRVpsbZ4"
      },
      "outputs": [],
      "source": [
        "# Disable SSL verification\n",
        "ssl._create_default_https_context = ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Step 1: Create grayscale dataset\n",
        "def rgb_to_gray(image):\n",
        "    # Take the mean along the channel dimension to convert to grayscale\n",
        "    return torch.mean(image, dim=0, keepdim=True)\n",
        "\n",
        "train_data = CIFAR10(root='./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = CIFAR10(root='./datasets', train=False, download=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Apply the grayscale conversion function to the datasets\n",
        "train_data_gray = [(rgb_to_gray(image), label) for image, label in train_data]\n",
        "test_data_gray = [(rgb_to_gray(image), label) for image, label in test_data]\n",
        "\n",
        "original_train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=True)\n",
        "train_dataloader = DataLoader(train_data_gray, batch_size=64, shuffle=True, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_data_gray, batch_size=64, shuffle=False, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkK9aHBXwk0o"
      },
      "outputs": [],
      "source": [
        "# Initialize the colorizing neural network\n",
        "colorizing_net = ColorizingNet()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(colorizing_net.parameters(), lr=0.001)\n",
        "\n",
        "# Training the colorizing neural network\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    colorizing_net.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    colorizing_net.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for grayscale_image, original_image in zip(train_dataloader, original_train_dataloader):\n",
        "        grayscale_image = grayscale_image.to(device)\n",
        "        original_image = original_image.to(device)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(type(grayscale_image))\n",
        "        print(grayscale_image)\n",
        "        grayscale_image = torch.tensor(grayscale_image)\n",
        "        print(type(grayscale_image))\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = colorizing_net(grayscale_image)\n",
        "        loss = criterion(outputs, original_image)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_dataloader)}\")\n",
        "\n",
        "print(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjTMnTu4woPC"
      },
      "outputs": [],
      "source": [
        "# Step 3: Calculate the mean squared loss between the new colored image and the original image\n",
        "colorizing_net.eval()\n",
        "total_loss = 0.0\n",
        "num_batches = len(test_dataloader)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for grayscale_image, original_image in test_dataloader:\n",
        "        inputs = grayscale_image.view(-1, 1)\n",
        "        outputs = colorizing_net(inputs)\n",
        "        loss = criterion(outputs, original_image)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "mean_squared_loss = total_loss / num_batches\n",
        "print(f\"Mean Squared Loss on Test Data: {mean_squared_loss}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
